/**
 * LLM API å®¢æˆ·ç«¯ - æ”¯æŒ OpenAI èŒƒå¼çš„å„ç§ LLM æœåŠ¡
 */

export class LLMClient {
  constructor(config = {}) {
    this.apiUrl = config.apiUrl || '';
    this.apiKey = config.apiKey || '';
    this.model = config.model || 'gpt-4';
    this.maxTokens = config.maxTokens || 1000;
    this.temperature = config.temperature || 0.7;
    this.timeout = config.timeout || 30000; // 30ç§’è¶…æ—¶
    this.maxRetries = config.maxRetries || 3;
    this.retryDelay = config.retryDelay || 1000; // 1ç§’é‡è¯•å»¶è¿Ÿ
  }

  /**
   * å‘é€èŠå¤©å®Œæˆè¯·æ±‚
   * @param {Array} messages - æ¶ˆæ¯æ•°ç»„
   * @param {Object} options - å¯é€‰å‚æ•°
   * @returns {Promise} APIå“åº”
   */
  async chatCompletion(messages, options = {}) {
    // å¦‚æœæ²¡æœ‰API URLæˆ–API Keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
    if (!this.apiUrl || !this.apiKey) {
      console.log('[LLM Client] Using mock mode - no API credentials provided');
      return this.generateMockResponse(messages);
    }
    
    const requestBody = {
      model: options.model || this.model,
      messages: messages,
      max_tokens: options.maxTokens || this.maxTokens,
      temperature: options.temperature || this.temperature,
      stream: options.stream || false,
      ...options.extraParams
    };

    const requestOptions = {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`,
        ...options.headers
      },
      body: JSON.stringify(requestBody)
    };

    // æ‰“å°å®Œæ•´çš„è¯·æ±‚ä¿¡æ¯
    console.log('ğŸš€ [LLM Request] ===== å®Œæ•´è¯·æ±‚ä¿¡æ¯ =====');
    console.log(`ğŸ“ URL: ${this.apiUrl}/chat/completions`);
    console.log(`ğŸ”‘ Headers:`, {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${this.apiKey.substring(0, 10)}...`,
      ...options.headers
    });
    console.log(`ğŸ“ Request Body:`, JSON.stringify(requestBody, null, 2));
    console.log('=====================================');

    return this.makeRequestWithRetry(`${this.apiUrl}/chat/completions`, requestOptions);
  }

  /**
   * å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚
   * @param {string} url - è¯·æ±‚URL
   * @param {Object} options - è¯·æ±‚é€‰é¡¹
   * @returns {Promise} å“åº”ç»“æœ
   */
  async makeRequestWithRetry(url, options) {
    let lastError = null;
    
    for (let attempt = 1; attempt <= this.maxRetries; attempt++) {
      try {
        console.log(`[LLM Client] Attempt ${attempt}/${this.maxRetries} to ${url}`);
        
        const response = await this.makeRequestWithTimeout(url, options);
        
        if (!response.ok) {
          const errorText = await response.text();
          
          // æ‰“å°é”™è¯¯å“åº”ä¿¡æ¯
          console.log('âŒ [LLM Error Response] ===== é”™è¯¯å“åº”ä¿¡æ¯ =====');
          console.log(`ğŸ“Š Status: ${response.status} ${response.statusText}`);
          console.log(`ğŸ¯ Response Headers:`, Object.fromEntries(response.headers.entries()));
          console.log(`âš ï¸ Error Body:`, errorText);
          console.log('===========================================');
          
          const error = new Error(`HTTP ${response.status}: ${errorText}`);
          error.status = response.status;
          error.response = response;
          throw error;
        }

        const data = await response.json();
        
        // æ‰“å°å®Œæ•´çš„å“åº”ä¿¡æ¯
        console.log('âœ… [LLM Response] ===== å®Œæ•´å“åº”ä¿¡æ¯ =====');
        console.log(`ğŸ“Š Status: ${response.status} ${response.statusText}`);
        console.log(`ğŸ¯ Response Headers:`, Object.fromEntries(response.headers.entries()));
        console.log(`ğŸ“‹ Response Body:`, JSON.stringify(data, null, 2));
        console.log('======================================');
        
        console.log(`[LLM Client] Request successful on attempt ${attempt}`);
        return data;
        
      } catch (error) {
        lastError = error;
        console.error(`[LLM Client] Attempt ${attempt} failed:`, error.message);
        
        // å¦‚æœæ˜¯å®¢æˆ·ç«¯é”™è¯¯ï¼ˆ4xxï¼‰ï¼Œä¸é‡è¯•
        if (error.status && error.status >= 400 && error.status < 500) {
          throw error;
        }
        
        // æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
        if (attempt === this.maxRetries) {
          break;
        }
        
        // æŒ‡æ•°é€€é¿å»¶è¿Ÿ
        const delay = this.retryDelay * Math.pow(2, attempt - 1);
        console.log(`[LLM Client] Waiting ${delay}ms before retry...`);
        await this.sleep(delay);
      }
    }
    
    throw lastError;
  }

  /**
   * å¸¦è¶…æ—¶çš„è¯·æ±‚
   * @param {string} url - è¯·æ±‚URL
   * @param {Object} options - è¯·æ±‚é€‰é¡¹
   * @returns {Promise} fetchå“åº”
   */
  async makeRequestWithTimeout(url, options) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);
    
    try {
      const response = await fetch(url, {
        ...options,
        signal: controller.signal
      });
      return response;
    } catch (error) {
      if (error.name === 'AbortError') {
        throw new Error(`Request timeout after ${this.timeout}ms`);
      }
      throw error;
    } finally {
      clearTimeout(timeoutId);
    }
  }

  /**
   * è§£æLLMå“åº”ï¼Œæå–JSONå†…å®¹
   * @param {Object} response - LLM APIå“åº”
   * @returns {Object} è§£æåçš„æ•°æ®
   */
  parseResponse(response) {
    try {
      if (!response.choices || response.choices.length === 0) {
        throw new Error('No choices in response');
      }

      const content = response.choices[0].message.content;
      console.log(`[LLM Client] Raw response:`, content);

      // å°è¯•æå–JSONå†…å®¹
      const jsonMatch = content.match(/```json\s*([\s\S]*?)\s*```/) || 
                       content.match(/```\s*([\s\S]*?)\s*```/) ||
                       content.match(/\{[\s\S]*\}/);
      
      if (jsonMatch) {
        const jsonStr = jsonMatch[1] || jsonMatch[0];
        const parsed = JSON.parse(jsonStr);
        console.log(`[LLM Client] Parsed JSON:`, parsed);
        return parsed;
      }

      // å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå†…å®¹
      try {
        const parsed = JSON.parse(content);
        console.log(`[LLM Client] Direct parsed JSON:`, parsed);
        return parsed;
      } catch {
        // å¦‚æœéƒ½è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹
        console.warn(`[LLM Client] Failed to parse JSON, returning raw content`);
        return {
          answer: "æ²¡æœ‰å…³ç³»",
          score: 1,
          feedback: "å›ç­”æ ¼å¼æœ‰è¯¯ï¼Œè¯·é‡æ–°æé—®",
          progress: 0
        };
      }
    } catch (error) {
      console.error(`[LLM Client] Error parsing response:`, error);
      return {
        answer: "æ²¡æœ‰å…³ç³»", 
        score: 1,
        feedback: "ç³»ç»Ÿå¤„ç†é”™è¯¯ï¼Œè¯·é‡æ–°æé—®",
        progress: 0
      };
    }
  }

  /**
   * éªŒè¯å“åº”æ ¼å¼
   * @param {Object} parsed - è§£æåçš„å“åº”
   * @returns {Object} éªŒè¯å¹¶ä¿®æ­£åçš„å“åº”
   */
  validateResponse(parsed) {
    const validated = {
      answer: "æ²¡æœ‰å…³ç³»",
      score: 1,
      feedback: "æ— æ•ˆå›ç­”",
      progress: 0,
      hint: undefined
    };

    // éªŒè¯answerå­—æ®µ
    const validAnswers = ["æ˜¯", "ä¸æ˜¯", "æ²¡æœ‰å…³ç³»"];
    if (parsed.answer && validAnswers.includes(parsed.answer)) {
      validated.answer = parsed.answer;
    }

    // éªŒè¯scoreå­—æ®µ
    if (typeof parsed.score === 'number' && parsed.score >= 1 && parsed.score <= 10) {
      validated.score = Math.round(parsed.score);
    }

    // éªŒè¯feedbackå­—æ®µ
    if (typeof parsed.feedback === 'string' && parsed.feedback.length > 0) {
      validated.feedback = parsed.feedback.slice(0, 100); // é™åˆ¶é•¿åº¦
    }

    // éªŒè¯progresså­—æ®µ
    if (typeof parsed.progress === 'number' && parsed.progress >= 0 && parsed.progress <= 100) {
      validated.progress = Math.round(parsed.progress);
    }

    // éªŒè¯hintå­—æ®µ
    if (parsed.hint && typeof parsed.hint === 'string' && parsed.hint.length > 0) {
      validated.hint = parsed.hint.slice(0, 50); // é™åˆ¶é•¿åº¦
    }

    console.log(`[LLM Client] Validated response:`, validated);
    return validated;
  }

  /**
   * æµ‹è¯•APIè¿æ¥
   * @returns {Promise<boolean>} è¿æ¥æ˜¯å¦æˆåŠŸ
   */
  async testConnection() {
    try {
      const response = await this.chatCompletion([
        { role: "user", content: "æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'" }
      ], { maxTokens: 50 });
      
      return response && response.choices && response.choices.length > 0;
    } catch (error) {
      console.error('[LLM Client] Connection test failed:', error);
      return false;
    }
  }

  /**
   * Sleepå‡½æ•°
   * @param {number} ms - æ¯«ç§’æ•°
   * @returns {Promise} Promiseå¯¹è±¡
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * è·å–å®¢æˆ·ç«¯é…ç½®ä¿¡æ¯
   * @returns {Object} é…ç½®ä¿¡æ¯
   */
  getConfig() {
    return {
      apiUrl: this.apiUrl,
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
      timeout: this.timeout,
      maxRetries: this.maxRetries,
      hasApiKey: !!this.apiKey
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿå“åº”ï¼ˆç”¨äºæ¼”ç¤ºå’Œå¼€å‘ï¼‰
   * @param {Array} messages - æ¶ˆæ¯æ•°ç»„
   * @returns {Promise} æ¨¡æ‹Ÿçš„APIå“åº”
   */
  async generateMockResponse(messages) {
    // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 1000));
    
    // æå–ç”¨æˆ·çš„é—®é¢˜
    const userMessage = messages.find(msg => msg.role === 'user');
    if (!userMessage) {
      throw new Error('No user message found');
    }
    
    const question = this.extractQuestionFromPrompt(userMessage.content);
    
    // ç”Ÿæˆæ™ºèƒ½çš„æ¨¡æ‹Ÿå“åº”
    const mockResponse = this.generateIntelligentResponse(question);
    
    console.log(`[LLM Mock] Generated response for question: "${question}"`);
    console.log(`[LLM Mock] Response:`, mockResponse);
    
    return {
      choices: [{
        message: {
          role: 'assistant',
          content: JSON.stringify(mockResponse)
        }
      }],
      usage: {
        prompt_tokens: userMessage.content.length / 4,
        completion_tokens: JSON.stringify(mockResponse).length / 4,
        total_tokens: (userMessage.content.length + JSON.stringify(mockResponse).length) / 4
      }
    };
  }

  /**
   * ä»æç¤ºè¯ä¸­æå–é—®é¢˜
   * @param {string} prompt - å®Œæ•´çš„æç¤ºè¯
   * @returns {string} æå–å‡ºçš„é—®é¢˜
   */
  extractQuestionFromPrompt(prompt) {
    // æŸ¥æ‰¾ "ç©å®¶é—®é¢˜ï¼š" åé¢çš„å†…å®¹
    const questionMatch = prompt.match(/\*\*ç©å®¶é—®é¢˜ï¼š\*\*(.+?)(?:\n|$)/);
    if (questionMatch) {
      return questionMatch[1].trim();
    }
    
    // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€è¡Œä½œä¸ºé—®é¢˜
    const lines = prompt.split('\n').filter(line => line.trim());
    return lines[lines.length - 1] || 'æœªçŸ¥é—®é¢˜';
  }

  /**
   * ç”Ÿæˆæ™ºèƒ½çš„æ¨¡æ‹Ÿå“åº”
   * @param {string} question - ç”¨æˆ·é—®é¢˜
   * @returns {Object} æ ¼å¼åŒ–çš„å“åº”å¯¹è±¡
   */
  generateIntelligentResponse(question) {
    const answers = ['æ˜¯', 'ä¸æ˜¯', 'æ²¡æœ‰å…³ç³»'];
    const answer = answers[Math.floor(Math.random() * answers.length)];
    
    // åŸºäºé—®é¢˜å†…å®¹çš„æ™ºèƒ½è¯„åˆ†
    let score = 5; // åŸºç¡€åˆ†æ•°
    let feedback = 'ä¸€èˆ¬çš„é—®é¢˜';
    let progress = Math.floor(Math.random() * 30) + 10; // 10-40ä¹‹é—´çš„è¿›åº¦
    
    const questionLower = question.toLowerCase();
    
    // æ£€æŸ¥é—®é¢˜è´¨é‡å¹¶è°ƒæ•´åˆ†æ•°
    if (questionLower.includes('ä¸ºä»€ä¹ˆ') || questionLower.includes('æ€ä¹ˆ') || questionLower.includes('å¦‚ä½•')) {
      score = Math.floor(Math.random() * 2) + 8; // 8-9åˆ†
      feedback = 'å¾ˆå¥½çš„é—®é¢˜ï¼Œç›´å‡»æ ¸å¿ƒï¼';
      progress += 15;
    } else if (questionLower.includes('æ˜¯å¦') || questionLower.includes('æ˜¯ä¸æ˜¯') || question.endsWith('å—ï¼Ÿ') || question.endsWith('å—')) {
      score = Math.floor(Math.random() * 2) + 6; // 6-7åˆ†
      feedback = 'ä¸é”™çš„å°è¯•ï¼Œæ–¹å‘æ­£ç¡®';
      progress += 10;
    } else if (question.length < 5) {
      score = Math.floor(Math.random() * 2) + 2; // 2-3åˆ†
      feedback = 'é—®é¢˜å¤ªç®€å•ï¼Œéœ€è¦æ›´è¯¦ç»†';
      progress += 5;
    } else if (questionLower.includes('æ­»') || questionLower.includes('æ€') || questionLower.includes('å‡¶æ‰‹')) {
      score = Math.floor(Math.random() * 2) + 7; // 7-8åˆ†  
      feedback = 'æ¥è¿‘çœŸç›¸äº†ï¼';
      progress += 12;
    }
    
    // ç¡®ä¿è¿›åº¦åœ¨åˆç†èŒƒå›´å†…
    progress = Math.min(85, Math.max(5, progress));
    
    // å¶å°”ç»™ä¸€äº›æç¤º
    let hint = null;
    if (progress > 60 && Math.random() < 0.3) {
      const hints = [
        'æ³¨æ„å…³é”®äººç‰©çš„èŒä¸š',
        'æ€è€ƒç‰©å“çš„ç‰¹æ®Šç”¨é€”',
        'è€ƒè™‘æ—¶é—´å’Œåœ°ç‚¹çš„å…³ç³»',
        'å…³æ³¨ç»†èŠ‚ä¸­çš„çŸ›ç›¾',
        'ä»å¿ƒç†è§’åº¦åˆ†æåŠ¨æœº'
      ];
      hint = hints[Math.floor(Math.random() * hints.length)];
    }
    
    const response = {
      answer: answer,
      score: score,
      feedback: feedback,
      progress: progress
    };
    
    if (hint) {
      response.hint = hint;
    }
    
    return response;
  }
}

/**
 * åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹
 * @param {Object} env - ç¯å¢ƒå˜é‡å¯¹è±¡
 * @returns {LLMClient} LLMå®¢æˆ·ç«¯å®ä¾‹
 */
export function createLLMClient(env) {
  const config = {
    apiUrl: env.LLM_API_URL || '',
    apiKey: env.LLM_API_KEY || '',
    model: env.LLM_MODEL || 'gpt-4',
    maxTokens: parseInt(env.LLM_MAX_TOKENS) || 1000,
    temperature: parseFloat(env.LLM_TEMPERATURE) || 0.7,
    timeout: parseInt(env.LLM_TIMEOUT) || 30000,
    maxRetries: parseInt(env.LLM_MAX_RETRIES) || 3
  };

  console.log(`[LLM Client] Creating client with config:`, {
    ...config,
    apiKey: config.apiKey ? '***hidden***' : 'not set'
  });

  return new LLMClient(config);
}